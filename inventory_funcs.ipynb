{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Functions being created for automization of certain steps in checking and updating inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to check if files exist and if so, put them into a new subdirectory and mark them as downloaded in the spreadsheet\n",
    "### This WILL NOT automatically save the new dataframe to a csv!! \n",
    "### This WILL update the copy of the df with codes in the `Download` column\n",
    "    ### `Y` : downloaded    `N` : not downloaded \n",
    "def check_for_files(submodel_df, codes,axiom_dir, submodel_dir):\n",
    "\n",
    "    for code in codes: #loops through each unique code\n",
    "\n",
    "        if code != \"NA\": #any codes that have `NA` will be skipped \n",
    "            file_path_processed = glob.glob(f\"{axiom_dir}{code}.gpkg\") #axiom data came as processed and raw data so we will check for both versions\n",
    "            file_path_raw = glob.glob(f\"{axiom_dir}{code}_raw.gpkg\") #raw data\n",
    "\n",
    "            if len(file_path_raw) == 1: #check if the data file is there\n",
    "                shutil.copy(file_path_raw[0], submodel_dir) #copy the file to the new subd\n",
    "                submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"Downloaded\"] = \"Y\"\n",
    "\n",
    "            if len(file_path_processed) == 1: #same thing but for processed data\n",
    "                shutil.copy(file_path_processed[0], submodel_dir)\n",
    "\n",
    "            if len(file_path_raw) == 0: #set download to N since there was no file found; only looking for raw because we would only have processed data if we also have raw\n",
    "                submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"Downloaded\"] = \"N\"\n",
    "\n",
    "        if code == \"NA\":\n",
    "            submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"Downloaded\"] = \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to check if the downloaded data is empty, and if it is not empty, where there is overlap at both the regional and study-area scale ###\n",
    "### This WILL NOT automatically save the new dataframe to a csv!!\n",
    "### This WILL update the `downloaded` column with marker `E` if the data was marked as downloaded but is empty, and update the `study_region_overlap` and `study_area_overlap` columns\n",
    "    ### Outpout in each column is a list string of study areas or regions where there was overlap\n",
    "    ### `None` : no overlap      \n",
    "def check_coverage(submodel_df, study_areas, codes, submodel_dir, crs):\n",
    "    \n",
    "    for code in codes:\n",
    "        \n",
    "        if code != \"NA\" and submodel_df.loc[fisheries[\"UNIQUE_ID\"]== code][\"Downloaded\"].values == \"Y\": #only loop through rows in df that have a code AND that have data downloaded\n",
    "\n",
    "            data_layer = gpd.read_file(f\"{submodel_dir}{code}_raw.gpkg\").to_crs(crs) #raw data; project to the same crs as the study area polygon\n",
    "            \n",
    "            if data_layer.shape[0] == 0:\n",
    "                \n",
    "                submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"Downloaded\"] = \"E\" #Update so we know that the data was attempted to be downloaded but the gpkg is empty\n",
    "                submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"study_area_overlap\"] = \"None\" #Update so we know there is no overlap\n",
    "                submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"study_region_overlap\"] = \"None\" #Update so we know there is no overlap\n",
    "           \n",
    "            if data_layer.shape[0] > 0:\n",
    "                \n",
    "                data_layer = data_layer.loc[data_layer.geometry.geometry.type=='Polygon'] #We probably should not keep this but one of the layers I ran this on had a single geometry that was a line?? So I filtered it out lol\n",
    "                                                                                          #Not best practice but code was getting mad that the layer had multiple geometry types\n",
    "                overlay = study_areas.overlay(data_layer, how='intersection') #Overlay analysis to see where there is intersection\n",
    "                study_area_coverage = overlay[\"portName\"].unique() #get unique `portName` (these are the study site names) from overlay output\n",
    "                study_region_coverage = overlay[\"region\"].unique() #get unique `region` (region names) from overlay output\n",
    "\n",
    "                if study_area_coverage.shape[0] == 0: #if there was no intersection of the study areas and the data, input `None` as overlap value\n",
    "                    submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"study_area_overlap\"] = \"None\"\n",
    "                    submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"study_region_overlap\"] = \"None\"\n",
    "\n",
    "                else: #if there were areas of overlap, create a string of values of the overlap areas\n",
    "                    submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"study_area_overlap\"] = \",\".join(str(element) for element in study_area_coverage) #create string of study area overlap locations and add to original df of all data layers\n",
    "                    submodel_df.loc[submodel_df[\"UNIQUE_ID\"] == code, \"study_region_overlap\"] = \",\".join(str(element) for element in study_region_coverage) #create string of study region overlap locations and add to original df of all data layers\n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Examples for use of above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv version of the submodel inventory (convert to csv and make sure all column headings have `_` between words; example: https://docs.google.com/spreadsheets/d/1F_F7vWr95jUbVkLp3WgpGT1VImV4VhzNRt2VZLnVhqM/edit?usp=sharing)\n",
    "fisheries_csv = \"C:/Alaska/Fisheries/fisheries_spreadsheet.csv\"\n",
    "fisheries = gpd.read_file(fisheries_csv) #read csv\n",
    "codes = fisheries[\"UNIQUE_ID\"].unique() #getting all unique ID values\n",
    "\n",
    "#shapefile of the study areas\n",
    "study_areas_shp = \"C:/Alaska/study_areas\"\n",
    "study_areas = gpd.read_file(study_areas_shp)\n",
    "\n",
    "#directory where the original data is\n",
    "axiom_dir = \"C:/Alaska/Axiom_data/tables/\"\n",
    "#new subdirectory where you want the data to go\n",
    "submodel_dir = \"C:/Alaska/Fisheries/\"\n",
    "\n",
    "crs = \"EPSG:4269\" #crs of the study area file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: check_for_files()\n",
    "\n",
    "Function that checks if each file exists according to `UNIQUE_ID` \n",
    "populates the `Downloaded` column in the dfdependent on if the data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_files(fisheries,codes,axiom_dir,submodel_dir) #run the check_for_files func on the fisheries submodel\n",
    "\n",
    "fisheries.tail() #look at bottom of fisheries df; you can see if it updated the df `downloaded` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: check if file had data, and if data area overlaps with study areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_coverage(fisheries, study_areas, codes, submodel_dir) #run the check_coverage func on the fisheries submodel\n",
    "\n",
    "fisheries.tail() #look at bottom of fisheries df; you can see if it updated the `study_region_overlap` and `study_area_overlap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download final df as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisheries.to_csv(f'{submodel_dir}fisheries_test.csv') #export the df as a csv to preserve changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
